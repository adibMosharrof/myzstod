pretrain_epochs: 1
train_epochs: 1
pretrain_batch_size: 2
train_batch_size: 2
num_workers: 1
data_split_percent:
- .1
- 1
- .1
eval_batch_size: 4
eval_steps: 100
test_batch_size: 20
test_prompt_max_len: 750
max_token_len: 1024
num_dialogs:
- 1
- 1
- 1
# overwrite:
#   - true
#   - true
#   - true
should_test: true
test_domain_settings:
- all
wandb:
  project: ZSTodLocal
  entity: adibm
  notes: mh base
train_domain_setting: seen
is_multi_head: true
should_add_schema: true
should_add_sys_actions: true
should_add_user_actions: true
should_add_service_results: true
model_name: distilgpt2
fp16: true
gradient_accumulation_steps: 4
