pretrain_epochs: 25
train_epochs: 10
pretrain_batch_size: 5
train_batch_size: 1
data_split_percent:
- 1
- 1
- 1
eval_batch_size: 10
eval_steps: 50
test_batch_size: 8
test_prompt_max_len: 750
max_token_len: 1024
# num_dialogs:
# - 1
# - 1
# - 1
num_dialogs:
- 127
- 20
- 34
# overwrite:
#   - true
#   - true
#   - true
should_test: true
train_domain_settings: 
  - seen
dev_domain_settings: 
  - all
test_domain_settings:
  - [all]
  - [seen]
  - [unseen]
  # - [Movies_1]
create_data_from_train: false
# create_data_from_train_splits:
#   - 0.01
#   - 0.01
wandb:
  project: ZSTodLocal
  entity: adibm
  notes: llama 7b full dataset
  task: arithmetic
# model_name: facebook/opt-6.7b
# model_name: facebook/opt-2.7b
# model_name: huggyllama/llama-30b
model_name: huggyllama/llama-7b
postprocess_generation: true
fp16: false
gradient_accumulation_steps: 64
eval_accumulation_steps: 62
quantization: true
two_step_training: false
should_add_special_tokens: true
save_wte: true
is_multi_task: true
multi_tasks:
  - dsts
  - actions
  - nlg
# context_type: default
should_add_schema: true
should_add_sys_actions: true
should_add_user_actions: true
should_add_service_results: true