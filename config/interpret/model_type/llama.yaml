model_name: meta-llama/Llama-3.2-1B-Instruct
# model_path: ""
model_path: outputs/interpret/2025-02-19/17-46-09_llama1b/results/checkpoint-2
train_batch_size: 5
eval_batch_size: 5
test_batch_size: 20
gradient_accumulation_steps: 32
eval_accumulation_steps: 32
quantization: false
quantization_dtype: 4
learning_rate: 1e-3
model_log_name: llama1b
context_type: gpt_api_call