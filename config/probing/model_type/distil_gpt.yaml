model_name: distilbert/distilgpt2
model_path: ""
train_batch_size: 30
eval_batch_size: 40
test_batch_size: 60
gradient_accumulation_steps: 32
eval_accumulation_steps: 32
quantization: false
learning_rate: 1e-3
model_log_name: distilgpt2