Creating Task Oriented Dialog (TOD) Systems that generalize well to unseen domains has been a challenging research area in dialog.
Most systems use the dialog history as context, and as the number of turns in a dialog increases, the context becomes too long and in many cases contain
repetitive and unnecessary information. Also, the metrics used to evaluate dialog systems mainly focus on dialog state tracking (DST) and response generation, and seem to
neglect system actions. In this paper, we propose a novel TOD system that uses a compressed context consisting of the latest DST and the last user utterance.
Using a compressed context allows us to feed additional information like the schema, list of system actions, user actions and service results to the model, and still use
language models like GPT-2. We propose a two step training process, where in the first pass we calculate the cross entropy loss on the context and target, and
in the second pass we calculate the loss only on the target.
To get a better understanding of the system actions, we propose two new metrics called Average Action Accuracy (AAA) and Joint Action Accuracy (JAA).
Experimental results on the Schema Guided Dialogue (SGD) dataset show that our model outperforms the state of the art models in terms of zero shot generalization.
